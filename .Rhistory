cp_=matrix(0,4,5)
for (i in 1:4) {
for (j in 1:5) {
rp_Cla <- rpart(formula_Tit,data_train,method = "class",minsplit=minsplit[i],xval=xval[j],cp=0.1)
cp_[i,j]=rp_Cla$cptable[which.min(rp_Cla$cptable[,"xerror"]),"CP"]
rp_Cla=prune(rp_Cla,cp=cp_[i,j])
pre=predict(rp_Cla,data_test,type = "class")
pre_false[i,j]=sum(pre!=data_test$Survived)
}
}
dimnames(pre_false)=list(minsplit,xval)
pre_false
cp_
i=2
j=1
rp_Cla <- rpart(formula_Tit,data_train,method = "class",minsplit=minsplit[i],xval=xval[j],cp=0.1)
rp_Cla
summary(rp_Cla)
minsplit=c(10,20,30,50)
xval=c(10,30,50,100,150)
pre_false=matrix(0,4,5)
cp_=matrix(0,4,5)
for (i in 1:4) {
for (j in 1:5) {
rp_Cla <- rpart(formula_Tit,data_train,method = "class",minsplit=minsplit[i],xval=xval[j],cp=0.01)
cp_[i,j]=rp_Cla$cptable[which.min(rp_Cla$cptable[,"xerror"]),"CP"]
rp_Cla=prune(rp_Cla,cp=cp_[i,j])
pre=predict(rp_Cla,data_test,type = "class")
pre_false[i,j]=sum(pre!=data_test$Survived)
}
}
dimnames(pre_false)=list(minsplit,xval)
pre_false
minsplit=c(10,20,30,50)
xval=c(10,30,50,100,150)
pre_false=matrix(0,4,5)
cp=matrix(0,4,5)
for (i in 1:4) {
for (j in 1:5) {
rp_Cla <- rpart(formula_Tit,data_train,method = "class",minsplit=minsplit[i],xval=xval[j],cp=0.01)
cp[i,j]=rp_Cla$cptable[which.min(rp_Cla$cptable[,"xerror"]),"CP"]
rp_Cla=prune(rp_Cla,cp=cp[i,j])
pre=predict(rp_Cla,data_test,type = "class")
pre_false[i,j]=sum(pre!=data_test$Survived)
}
}
dimnames(pre_false)=list(minsplit,xval)
pre_false
cp
i=2
j=1
minsplit=c(10,20,30,50)
xval=c(10,30,50,100,150)
pre_false=matrix(0,4,5)
cp=matrix(0,4,5)
rp_Cla <- rpart(formula_Tit,data_train,method = "class",minsplit=minsplit[i],xval=xval[j],cp=0.01)
summary(rp_Cla)
rp_Cla$cptable[which.min(rp_Cla$cptable[,"xerror"]),"CP"]
cp[i,j]=rp_Cla$cptable[which.min(rp_Cla$cptable[,"xerror"]),"CP"]
cp
rp_Cla=prune(rp_Cla,cp=cp[i,j])
pre=predict(rp_Cla,data_test,type = "class")
pre_false[i,j]=sum(pre!=data_test$Survived)
dimnames(pre_false)=list(minsplit,xval)
pre_false
cp
minsplit=c(10,20,30,50)
xval=c(10,30,50,100,150)
pre_false=matrix(0,4,5)
cp=matrix(0,4,5)
for (i in 1:4) {
for (j in 1:5) {
rp_Cla <- rpart(formula_Tit,data_train,method = "class",minsplit=minsplit[i],xval=xval[j],cp=0.01)
cp[i,j]=rp_Cla$cptable[which.min(rp_Cla$cptable[,"xerror"]),"CP"]
rp_Cla=prune(rp_Cla,cp=cp[i,j])
pre=predict(rp_Cla,data_test,type = "class")
pre_false[i,j]=sum(pre!=data_test$Survived)
}
}
dimnames(pre_false)=list(minsplit,xval)
pre_false
cp
minsplit=c(10,20,30,50)
xval=c(10,30,50,100,150)
pre_false=matrix(0,4,5)
cp=c(0.01,0.02,0.03,0.04,0.05)
for (i in 1:4) {
for (j in 1:5) {
rp_Cla <- rpart(formula_Tit,data_train,method = "class",minsplit=minsplit[i],cp=cp[j])
pre=predict(rp_Cla,data_test,type = "class")
pre_false[i,j]=sum(pre!=data_test$Survived)
}
}
dimnames(pre_false)=list(minsplit,xval)
pre_false
formula_Tit=Survived~.
rp_Cla=rpart(formula_Tit,data_train,method = "class",minsplit=20,cp=0.01,xval=10) #預設值 #minsplit表示每個節點中所包含樣本數的最小值，cp是複雜度參數
rp_Cla$cptable
cp=rp_Cla$cptable[which.min(rp_Cla$cptable[,"xerror"]),"CP"] #選xerror最小的cp
rp_Cla=prune(rp_Cla,cp=cp)
rp_Cla
summary(rp_Cla)
rpart.plot(rp_Cla)
#--------測試-----------
pre_Cla=predict(rp_Cla,data_test,type = "class",cp=cp)
pre_Cla
table(data_test$Survived,pre_Cla)
(sum(diag(table(data_test$Survived,pre_Cla)))/sum(table(data_test$Survived,pre_Cla)))
minsplit=c(10,20,30,50)
xval=c(0,10,30,50,100)
pre_false=matrix(0,4,5)
cp=matrix(0,4,5)
for (i in 1:4) {
for (j in 1:5) {
rp_Cla <- rpart(formula_Tit,data_train,method = "class",minsplit=minsplit[i],xval=xval[j],cp=0.01)
cp[i,j]=rp_Cla$cptable[which.min(rp_Cla$cptable[,"xerror"]),"CP"]
rp_Cla=prune(rp_Cla,cp=cp[i,j])
pre=predict(rp_Cla,data_test,type = "class")
pre_false[i,j]=sum(pre!=data_test$Survived)
}
}
dimnames(pre_false)=list(minsplit,xval)
pre_false
rp_Cla=rpart(formula_Tit,data_train,method = "class",minsplit=20,cp=0.01,xval=0) #預設值 #minsplit表示每個節點中所包含樣本數的最小值，cp是複雜度參數
rp_Cla$cptable
?rpart.control
source('D:/R project/Statistical ML lab (NCCU)/dicision tree for discuss/dicision tree.R', encoding = 'UTF-8')
minsplit <- c(10, 20, 30, 50)
xval <- c(10, 30, 50, 100, 150) #xval若為0則代表沒有validation(也不會有xerror)
pre_false <- matrix(0, 4, 5)
cp <- matrix(0, 4, 5)
for (i in 1:4) {
for (j in 1:5) {
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = minsplit[i], xval = xval[j], cp = 0.01)
cp[i,j] <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"]
rp_Cla <- prune(rp_Cla, cp = cp[i,j])
pre <- predict(rp_Cla, data_test, type = "class")
pre_false[i,j] <- sum(pre != data_test$Survived)
}
}
cp
i=2
j=1
minsplit <- c(10, 20, 30, 50)
xval <- c(10, 30, 50, 100, 150) #xval若為0則代表沒有validation(也不會有xerror)
pre_false <- matrix(0, 4, 5)
cp <- matrix(0, 4, 5)
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = minsplit[i], xval = xval[j], cp = 0.01)
cp[i,j] <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"]
rp_Cla <- prune(rp_Cla, cp = cp[i,j])
pre <- predict(rp_Cla, data_test, type = "class")
pre_false[i,j] <- sum(pre != data_test$Survived)
cp
minsplit <- c(10, 20, 30, 50)
xval <- c(10, 30, 50, 100, 150) #xval若為0則代表沒有validation(也不會有xerror)
pre_false <- matrix(0, 4, 5)
cp <- matrix(0, 4, 5)
for (i in 1:4) {
for (j in 1:5) {
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = minsplit[i], xval = xval[j], cp = 0.01)
cp[i,j] <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"]
rp_Cla <- prune(rp_Cla, cp = cp[i,j])
pre <- predict(rp_Cla, data_test, type = "class")
pre_false[i,j] <- sum(pre != data_test$Survived)
}
}
dimnames(pre_false) <- list(minsplit, xval)
pre_false
cp
i=2
j=1
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = minsplit[i], xval = xval[j], cp = 0.01)
cp[i,j] <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"]
rp_Cla <- prune(rp_Cla, cp = cp[i,j])
pre <- predict(rp_Cla, data_test, type = "class")
pre_false[i,j] <- sum(pre != data_test$Survived)
cp
i=3
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = minsplit[i], xval = xval[j], cp = 0.01)
cp[i,j] <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"]
rp_Cla <- prune(rp_Cla, cp = cp[i,j])
pre <- predict(rp_Cla, data_test, type = "class")
pre_false[i,j] <- sum(pre != data_test$Survived)
cp
minsplit <- c(10, 20, 30, 50)
xval <- c(10, 30, 50, 100, 150) #xval若為0則代表沒有validation(也不會有xerror)
pre_false <- matrix(0, 4, 5)
cp <- matrix(0, 4, 5)
for (i in 1:4) {
for (j in 1:5) {
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = minsplit[i], xval = xval[j], cp = 0.01)
cp[i,j] <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"]
rp_Cla <- prune(rp_Cla, cp = cp[i,j])
pre <- predict(rp_Cla, data_test, type = "class")
pre_false[i,j] <- sum(pre != data_test$Survived)
}
}
cp
minsplit <- c(10, 20, 30, 50)
xval <- c(10, 30, 50, 100, 150) #xval若為0則代表沒有validation(也不會有xerror)
pre_false <- matrix(0, 4, 5)
cp <- matrix(0, 4, 5)
for (i in 1:4) {
for (j in 1:5) {
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = minsplit[i], xval = xval[j], cp = 0.01)
cp[i,j] <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"]
rp_Cla <- prune(rp_Cla, cp = cp[i,j])
pre <- predict(rp_Cla, data_test, type = "class")
pre_false[i,j] <- sum(pre != data_test$Survived)
}
}
cp
minsplit <- c(10, 20, 30, 50)
xval <- c(10, 30, 50, 100, 150) #xval若為0則代表沒有validation(也不會有xerror)
pre_false <- matrix(0, 4, 5)
cp <- matrix(0, 4, 5)
for (i in 1:4) {
for (j in 1:5) {
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = minsplit[i], xval = xval[j], cp = 0.01)
cp[i,j] <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"]
rp_Cla <- prune(rp_Cla, cp = cp[i,j])
pre <- predict(rp_Cla, data_test, type = "class")
pre_false[i,j] <- sum(pre != data_test$Survived)
}
}
cp
minsplit <- c(10, 20, 30, 50)
xval <- c(10, 30, 50, 100, 150) #xval若為0則代表沒有validation(也不會有xerror)
pre_false <- matrix(0, 4, 5)
cp <- matrix(0, 4, 5)
for (i in 1:4) {
for (j in 1:5) {
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = minsplit[i], xval = xval[j], cp = 0.01)
cp[i,j] <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"]
rp_Cla <- prune(rp_Cla, cp = cp[i,j])
pre <- predict(rp_Cla, data_test, type = "class")
pre_false[i,j] <- sum(pre != data_test$Survived)
}
}
cp
minsplit <- c(10, 20, 30, 50)
xval <- c(10, 30, 50, 100, 150) #xval若為0則代表沒有validation(也不會有xerror)
pre_false <- matrix(0, 4, 5)
cp <- matrix(0, 4, 5)
for (i in 1:4) {
for (j in 1:5) {
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = minsplit[i], xval = xval[j], cp = 0.01)
cp[i,j] <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"]
rp_Cla <- prune(rp_Cla, cp = cp[i,j])
pre <- predict(rp_Cla, data_test, type = "class")
pre_false[i,j] <- sum(pre != data_test$Survived)
}
}
cp
minsplit <- c(10, 20, 30, 50)
xval <- c(10, 30, 50, 100, 150) #xval若為0則代表沒有validation(也不會有xerror)
pre_false <- matrix(0, 4, 5)
cp <- matrix(0, 4, 5)
for (i in 1:4) {
for (j in 1:5) {
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = minsplit[i], xval = xval[j], cp = 0.01)
cp[i,j] <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"]
rp_Cla <- prune(rp_Cla, cp = cp[i,j])
pre <- predict(rp_Cla, data_test, type = "class")
pre_false[i,j] <- sum(pre != data_test$Survived)
}
}
cp
minsplit <- c(10, 20, 30, 50)
xval <- c(10, 30, 50, 100, 150) #xval若為0則代表沒有validation(也不會有xerror)
pre_false <- matrix(0, 4, 5)
cp <- matrix(0, 4, 5)
for (i in 1:4) {
for (j in 1:5) {
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = minsplit[i], xval = xval[j], cp = 0.01)
cp[i,j] <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"]
rp_Cla <- prune(rp_Cla, cp = cp[i,j])
pre <- predict(rp_Cla, data_test, type = "class")
pre_false[i,j] <- sum(pre != data_test$Survived)
}
}
cp
formula_Tit <- Survived~.
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = 20, cp = 0.01, xval = 0) #預設值 #minsplit表示每個節點中所包含樣本數的最小值，cp是複雜度參數
rp_Cla$cptable
cp <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"] #選xerror最小的cp
rp_Cla <- prune(rp_Cla, cp = cp)
rp_Cla
summary(rp_Cla)
rpart.plot(rp_Cla)
#--------測試-----------
pre_Cla <- predict(rp_Cla, data_test, type = "class", cp = cp)
pre_Cla
table(data_test$Survived, pre_Cla)
(sum(diag(table(data_test$Survived, pre_Cla))) / sum(table(data_test$Survived, pre_Cla)))
formula_Tit <- Survived~.
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = 20, cp = 0.01, xval = 0) #預設值 #minsplit表示每個節點中所包含樣本數的最小值，cp是複雜度參數
rp_Cla$cptable
cp <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"] #選xerror最小的cp
rp_Cla <- prune(rp_Cla, cp = cp)
rp_Cla
summary(rp_Cla)
rpart.plot(rp_Cla)
#--------測試-----------
pre_Cla <- predict(rp_Cla, data_test, type = "class", cp = cp)
pre_Cla
table(data_test$Survived, pre_Cla)
(sum(diag(table(data_test$Survived, pre_Cla))) / sum(table(data_test$Survived, pre_Cla)))
formula_Tit <- Survived~.
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = 20, cp = 0.01, xval = 0) #預設值 #minsplit表示每個節點中所包含樣本數的最小值，cp是複雜度參數
rp_Cla$cptable
cp <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"] #選xerror最小的cp
rp_Cla <- prune(rp_Cla, cp = cp)
rp_Cla
summary(rp_Cla)
rpart.plot(rp_Cla)
#--------測試-----------
pre_Cla <- predict(rp_Cla, data_test, type = "class", cp = cp)
pre_Cla
table(data_test$Survived, pre_Cla)
(sum(diag(table(data_test$Survived, pre_Cla))) / sum(table(data_test$Survived, pre_Cla)))
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(rpart.plot)
formula_Tit <- Survived~.
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = 20, cp = 0.01, xval = 0) #預設值 #minsplit表示每個節點中所包含樣本數的最小值，cp是複雜度參數
rp_Cla$cptable
cp <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"] #選xerror最小的cp
library(rpart)
library(rpart.plot)
formula_Tit <- Survived~.
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = 20, cp = 0.01, xval = 10) #預設值 #minsplit表示每個節點中所包含樣本數的最小值，cp是複雜度參數
rp_Cla$cptable
cp <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"] #選xerror最小的cp
rp_Cla <- prune(rp_Cla, cp = cp)
rp_Cla
summary(rp_Cla)
rpart.plot(rp_Cla)
library(rpart)
library(rpart.plot)
formula_Tit <- Survived~.
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = 20, cp = 0.01, xval = 10) #預設值 #minsplit表示每個節點中所包含樣本數的最小值，cp是複雜度參數
rp_Cla$cptable
cp <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"] #選xerror最小的cp
rp_Cla <- prune(rp_Cla, cp = cp)
rp_Cla
library(rpart)
library(rpart.plot)
formula_Tit <- Survived~.
rp_Cla <- rpart(formula_Tit, data_train, method = "class", minsplit = 20, cp = 0.01, xval = 10) #預設值 #minsplit表示每個節點中所包含樣本數的最小值，cp是複雜度參數
rp_Cla$cptable
cp <- rp_Cla$cptable[which.min(rp_Cla$cptable[, "xerror"]), "CP"] #選xerror最小的cp
rp_Cla <- prune(rp_Cla, cp = cp)
rp_Cla
summary(rp_Cla)
rpart.plot(rp_Cla)
library(magrittr)
data <- read.csv("data_train.csv")
var <- c("Pclass","Sex","Age","SibSp","Parch","Fare",
"Embarked","Title","FsizeD","Child","Mother")
data[1:10,var] %>% knitr::kable()
# 將原本的train資料再切割成training跟testing
# 利用caret套件裡的createDataPartition函式
# 依train$Survived欄位的0,1分佈比例，等比例做資料集切割
library(caret)
set.seed(200)
inTraining <- createDataPartition(data$Survived, p = .8, list = FALSE)
data_train <- data[ inTraining,c("Survived",var)]
data_test <- data[-inTraining,c("Survived",var)]
# 確認各資料集0,1出現比例
# 原始train資料集0,1分佈比例
prop.table(table(data$Survived)) * 100
# 切割過後的 training data 0,1分佈比例
prop.table(table(data_train$Survived)) * 100
data <- read.csv("data_train.csv")
var <- c("Pclass","Sex","Age","SibSp","Parch","Fare",
"Embarked","Title","FsizeD","Child","Mother")
data[1:10,var] %>% knitr::kable()
# 將原本的train資料再切割成training跟testing
# 利用caret套件裡的createDataPartition函式
# 依train$Survived欄位的0,1分佈比例，等比例做資料集切割
library(caret)
set.seed(200)
inTraining <- createDataPartition(data$Survived, p = .8, list = FALSE)
data_train <- data[ inTraining,c("Survived",var)]
data_test <- data[-inTraining,c("Survived",var)]
# 確認各資料集0,1出現比例
# 原始train資料集0,1分佈比例
prop.table(table(data$Survived)) * 100
# 切割過後的 training data 0,1分佈比例
prop.table(table(data_train$Survived)) * 100
library(magrittr)
data <- read.csv("data_train.csv")
var <- c("Pclass","Sex","Age","SibSp","Parch","Fare",
"Embarked","Title","FsizeD","Child","Mother")
data[1:10,var] %>% knitr::kable()
# 將原本的train資料再切割成training跟testing
# 利用caret套件裡的createDataPartition函式
# 依train$Survived欄位的0,1分佈比例，等比例做資料集切割
library(caret)
set.seed(200)
inTraining <- createDataPartition(data$Survived, p = .8, list = FALSE)
data_train <- data[ inTraining,c("Survived",var)]
data_test <- data[-inTraining,c("Survived",var)]
# 確認各資料集0,1出現比例
# 原始train資料集0,1分佈比例
prop.table(table(data$Survived)) * 100
# 切割過後的 training data 0,1分佈比例
prop.table(table(data_train$Survived)) * 100
# 切割過後的 testing data 0,1分佈比例
prop.table(table(data_test$Survived)) * 100
library("lattice", lib.loc="C:/Program Files/R/R-3.3.1/library")
library("ggplot2", lib.loc="~/R/win-library/3.3")
library(caret)
library(magrittr)
data <- read.csv("data_train.csv")
var <- c("Pclass","Sex","Age","SibSp","Parch","Fare",
"Embarked","Title","FsizeD","Child","Mother")
data[1:10,var] %>% knitr::kable()
# 將原本的train資料再切割成training跟testing
# 利用caret套件裡的createDataPartition函式
# 依train$Survived欄位的0,1分佈比例，等比例做資料集切割
library(caret)
set.seed(200)
inTraining <- createDataPartition(data$Survived, p = .8, list = FALSE)
data_train <- data[ inTraining,c("Survived",var)]
data_test <- data[-inTraining,c("Survived",var)]
?WOW(J48)
WOW(J48)
library(RWeka)
?WOW(J48)
WOW(J48)
library(RWeka)
data_train$Survived=as.factor(data_train$Survived) #將Survived改為因數型，使J48()函數可辨別
formula_Tit=Survived~.
C45=J48(formula_Tit,data_train)
C45
summary(C45)
plot(C45)
#--------測試-----------
pre_C45=predict(C45,data_test)
pre_C45
table(data_test$Survived,pre_C45)
(sum(diag(table(data_test$Survived,pre_Cla)))/sum(table(data_test$Survived,pre_Cla)))
M=c(2,3,4,5)
C=c(0.1,0.2,0.25,0.3,0.35)
pre_false=matrix(0,4,5)
for (i in 1:4) {
for (j in 1:5) {
pre=predict(J48(formula_Tit,data_train,control=Weka_control(M=M[i],C=C[j])),data_test)
pre_false[i,j]=sum(pre!=data_test$Survived)
}
}
dimnames(pre_false)=list(M,C)
pre_false
C45
plot(C45)
C=c(0.01,0.05,0.1,0.25)
M=c(2,5,10,20,30)
pre_false=matrix(0,4,5)
for (i in 1:4) {
for (j in 1:5) {
pre=predict(J48(formula_Tit,data_train,control=Weka_control(C=C[i]), M=M[j]),data_test)
pre_false[i,j]=sum(pre!=data_test$Survived)
}
}
dimnames(pre_false)=list(C,M)
pre_false
i=1
j=1
pre=predict(J48(formula_Tit,data_train,control=Weka_control(C=C[i]), M=M[j]),data_test)
C=c(0.01,0.05,0.1,0.25)
M=c(2,5,10,20,30)
pre_false=matrix(0,4,5)
for (i in 1:4) {
for (j in 1:5) {
pre=predict(J48(formula_Tit,data_train,control=Weka_control(C=C[i], M=M[j])),data_test)
pre_false[i,j]=sum(pre!=data_test$Survived)
}
}
dimnames(pre_false)=list(C,M)
pre_false
C45=J48(formula_Tit,data_train,control=Weka_control(M=10,C=0.05))
C45
plot(C45)
pre_C45=predict(C45,data_test)
pre_C45
table(data_test$Survived,pre_C45)
(sum(diag(table(data_test$Survived,pre_Cla)))/sum(table(data_test$Survived,pre_Cla)))
(sum(diag(table(data_test$Survived,pre_C45)))/sum(table(data_test$Survived,pre_C45)))
data_train$Survived=as.factor(data_train$Survived) #將Survived改為因數型，使J48()函數可辨別
formula_Tit=Survived~.
C45=J48(formula_Tit,data_train)
C45
summary(C45)
plot(C45)
pre_C45=predict(C45,data_test)
pre_C45
table(data_test$Survived,pre_C45)
(sum(diag(table(data_test$Survived,pre_C45)))/sum(table(data_test$Survived,pre_C45)))
C=c(0.01,0.05,0.1,0.25)
M=c(2,5,10,20,30)
pre_false=matrix(0,4,5)
for (i in 1:4) {
for (j in 1:5) {
pre=predict(J48(formula_Tit,data_train,control=Weka_control(C=C[i], M=M[j])),data_test)
pre_false[i,j]=sum(pre!=data_test$Survived)
}
}
dimnames(pre_false)=list(C,M)
pre_false
source('D:/R project/Statistical ML lab (NCCU)/dicision tree for discuss/dicision tree.R', encoding = 'UTF-8')
