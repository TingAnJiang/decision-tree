{
    "collab_server" : "",
    "contents" : "library(magrittr)\ndata <- read.csv(\"data_train.csv\")\n\nvar <- c(\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\n         \"Embarked\",\"Title\",\"FsizeD\",\"Child\",\"Mother\")\n\ndata[1:10,var] %>% knitr::kable()\n\n# 將原本的train資料再切割成training跟testing\n# 利用caret套件裡的createDataPartition函式\n# 依train$Survived欄位的0,1分佈比例，等比例做資料集切割\n\nlibrary(caret)\nset.seed(200)\ninTraining <- createDataPartition(data$Survived, p = .8, list = FALSE)\ndata_train <- data[ inTraining,c(\"Survived\",var)]\ndata_test <- data[-inTraining,c(\"Survived\",var)]\n\n\n# 確認各資料集0,1出現比例\n# 原始train資料集0,1分佈比例\nprop.table(table(data$Survived)) * 100\n# 切割過後的 training data 0,1分佈比例\nprop.table(table(data_train$Survived)) * 100\n# 切割過後的 testing data 0,1分佈比例\nprop.table(table(data_test$Survived)) * 100\n\n#CART\n#------------------開始種樹--------------------\nlibrary(rpart)\nlibrary(rpart.plot)\nformula_Tit <- Survived~.\nrp_Cla <- rpart(formula_Tit, data_train, method = \"class\", minsplit = 20, cp = 0.01, xval = 10) #預設值 #minsplit表示每個節點中所包含樣本數的最小值，cp是複雜度參數\nrp_Cla$cptable\ncp <- rp_Cla$cptable[which.min(rp_Cla$cptable[, \"xerror\"]), \"CP\"] #選xerror最小的cp\nrp_Cla <- prune(rp_Cla, cp = cp)\nrp_Cla\nsummary(rp_Cla)\nrpart.plot(rp_Cla)\n\n#--------測試-----------\npre_Cla <- predict(rp_Cla, data_test, type = \"class\", cp = cp)\npre_Cla\ntable(data_test$Survived, pre_Cla)\n\n(sum(diag(table(data_test$Survived, pre_Cla))) / sum(table(data_test$Survived, pre_Cla)))\n\n\n#---------------綜合建模--------------\nminsplit <- c(10, 20, 30, 50)\nxval <- c(10, 30, 50, 100, 150) #xval若為0則代表沒有validation(也不會有xerror)\npre_false <- matrix(0, 4, 5)\ncp <- matrix(0, 4, 5)\n\nfor (i in 1:4) {\n  for (j in 1:5) {\n    rp_Cla <- rpart(formula_Tit, data_train, method = \"class\", minsplit = minsplit[i], xval = xval[j], cp = 0.01)\n    cp[i,j] <- rp_Cla$cptable[which.min(rp_Cla$cptable[, \"xerror\"]), \"CP\"]\n    rp_Cla <- prune(rp_Cla, cp = cp[i,j])\n    pre <- predict(rp_Cla, data_test, type = \"class\")\n    pre_false[i,j] <- sum(pre != data_test$Survived)\n  }\n}\ncp\ndimnames(pre_false) <- list(minsplit, xval)\npre_false\ncp\n#取預設值就有一樣好的效果\n\n\n#C4.5\n#------------------開始種樹--------------------\nlibrary(RWeka)\ndata_train$Survived <- as.factor(data_train$Survived) #將Survived改為因數型，使J48()函數可辨別\nformula_Tit <- Survived~.\nC45 <- J48(formula_Tit, data_train) #先使用預設值種樹\nC45 #這棵樹種起來非常大顆\nsummary(C45)\nplot(C45)\n\n#--------測試-----------\npre_C45 <- predict(C45, data_test)\npre_C45\ntable(data_test$Survived, pre_C45)\n\n(sum(diag(table(data_test$Survived, pre_C45))) / sum(table(data_test$Survived, pre_C45)))\n\n\n#---------------綜合建模--------------\nC <- c(0.01, 0.05, 0.1, 0.25)\nM <- c(2, 5, 10, 20, 30)\n\npre_false <- matrix(0, 4, 5)\n\nfor (i in 1:4) {\n  for (j in 1:5) {\n    pre <- predict(J48(formula_Tit, data_train, control = Weka_control(C = C[i], M = M[j])), data_test)\n    pre_false[i,j] <- sum(pre != data_test$Survived)\n  }\n}\n\n\ndimnames(pre_false) <- list(C, M)\npre_false\n\nC45 <- J48(formula_Tit, data_train, control = Weka_control(M = 10, C = 0.05))\nC45\nplot(C45)",
    "created" : 1494733976228.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "368582025",
    "id" : "56187CB7",
    "lastKnownWriteTime" : 1504785003,
    "last_content_update" : 1504785003580,
    "path" : "D:/R project/Statistical ML lab (NCCU)/dicision tree for discuss/dicision tree.R",
    "project_path" : "dicision tree.R",
    "properties" : {
        "docOutlineVisible" : "0",
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : true,
    "source_window" : "",
    "type" : "r_source"
}